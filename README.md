Perfecto ğŸš€ Te preparo un **README.md** al estilo que mostraste, pero adaptado a **tu parcial** con la info que ya generamos (implementaciones, resultados y conclusiones).

AquÃ­ lo tienes listo para usar:

---

````markdown
# ğŸ“¸ Parallel Image Filtering - Midterm 1.1
**AnÃ¡lisis Comparativo de ProgramaciÃ³n Paralela en Filtrado de ImÃ¡genes PPM/PGM**

<img alt="C++" src="https://img.shields.io/badge/C++-11-blue.svg">
<img alt="OpenMP" src="https://img.shields.io/badge/OpenMP-4.5-green.svg">
<img alt="MPI" src="https://img.shields.io/badge/MPI-3.1-red.svg">
<img alt="Docker" src="https://img.shields.io/badge/Docker-Ready-2496ED.svg">
<img alt="License" src="https://img.shields.io/badge/License-Academic-yellow.svg">

---

## ğŸ¯ Pregunta de InvestigaciÃ³n
**Â¿CÃ³mo influye la programaciÃ³n paralela, utilizando OpenMP o Pthreads y MPI (en un entorno simulado con Docker), en el tiempo de ejecuciÃ³n del filtrado de imÃ¡genes PPM y PGM en comparaciÃ³n con una implementaciÃ³n secuencial?**

---

## ğŸ“‹ DescripciÃ³n del Proyecto
Este proyecto implementa y compara **4 paradigmas de programaciÃ³n paralela** aplicados al filtrado de imÃ¡genes **PPM** y **PGM**, evaluando su rendimiento frente a una versiÃ³n **secuencial baseline**.

---

## ğŸ§© Componentes Implementados
- ğŸ”„ **Secuencial**: ImplementaciÃ³n base orientada a objetos.  
- ğŸ§µ **Pthreads**: Memoria compartida con divisiÃ³n por regiones.  
- âš¡ **OpenMP**: ParalelizaciÃ³n automÃ¡tica con directivas `#pragma`.  
- ğŸŒ **MPI**: DistribuciÃ³n por filas horizontales entre procesos, simulada en **Docker**.  

---

## ğŸ¨ Filtros Implementados
- **Blur (Suavizado)** â†’ ConvoluciÃ³n 3x3 para reducir ruido.  
- **Laplace (Bordes)** â†’ Operador Laplaciano para detectar contornos.  
- **Sharpen (Realce)** â†’ Resalta detalles y bordes.  

---

## ğŸ“Š Paradigmas de ParalelizaciÃ³n
- **Secuencial** â†’ Procesamiento pÃ­xel por pÃ­xel.  
- **Pthreads** â†’ DivisiÃ³n de la imagen en 4 cuadrantes.  
- **OpenMP** â†’ ParalelizaciÃ³n de bucles con `collapse(2)`.  
- **MPI** â†’ Reparto de filas entre procesos.  

---

## ğŸ”¬ Sistema de MediciÃ³n
- â±ï¸ Timer de alta precisiÃ³n (CPU + Wall time).  
- ğŸ“ˆ CÃ¡lculo automÃ¡tico de *speedups*.  
- ğŸ“Š CSVs con mÃ©tricas detalladas.  
- ğŸ–¼ï¸ Almacenamiento de imÃ¡genes filtradas.  

---

## ğŸ› ï¸ CompilaciÃ³n y EjecuciÃ³n
### Prerrequisitos
- `g++` con soporte `-fopenmp`
- `mpic++` (OpenMPI)
- `docker` y `docker-compose` para la simulaciÃ³n distribuida

### CompilaciÃ³n
```bash
# Secuencial
g++ -std=c++11 -O2 -o filterer filterer.cpp ...

# Pthreads
g++ -std=c++11 -O2 -pthread -o pfilterer pfilterer.cpp ...

# OpenMP
g++ -std=c++11 -O2 -fopenmp -o opfilterer opfilterer.cpp ...

# MPI
mpic++ -std=c++11 -O2 -o mpifilterer mpiFilterer.cpp ...
````

### EjecuciÃ³n

```bash
./filterer lena.pgm lena_seq_blur.pgm --f blur
./pfilterer lena.pgm lena_pthread_blur.pgm --f blur
OMP_NUM_THREADS=4 ./opfilterer lena.pgm lena_openmp_blur.pgm --f blur
mpirun -n 4 ./mpifilterer lena.pgm lena_mpi_blur.pgm --f blur
```

---

## ğŸ“Š Resultados de Rendimiento

### â±ï¸ Tiempo total por imagen (ms)

| Imagen     | Secuencial | Pthreads | OpenMP (3 filtros) | MPI (2p) | MPI (4p) |
| ---------- | ---------- | -------- | ------------------ | -------- | -------- |
| damma.pgm  | 1894       | 2212     | 1601               | 4822     | 8100     |
| sulfur.pgm | 1643       | 1626     | 1159               | 3584     | 5736     |
| fruit.pgm  | 631        | 768      | 586                | 2235     | 3322     |
| lena.pgm   | 728        | 636      | 646                | 1430     | 2584     |
| puj.pgm    | 1905       | 1754     | 1292               | 4371     | 7400     |

---

### ğŸ† Speedups Promedio

* ğŸ§µ **Pthreads** â†’ 0.82x â€“ 1.14x
* âš¡ **OpenMP** â†’ 1.08x â€“ 1.47x
* ğŸŒ **MPI** â†’ 0.19x â€“ 0.51x

---

## ğŸ” Hallazgos Principales

âœ… **Factores que mejoran el rendimiento**

* ImÃ¡genes grandes â†’ mejor escalabilidad.
* Filtro Blur â†’ el mÃ¡s eficiente en paralelo.
* OpenMP logra balance Ã³ptimo entre facilidad y rendimiento.

âš ï¸ **Limitaciones**

* **MPI** presenta alto overhead en Docker (solo conviene en distribuidos reales).
* ImÃ¡genes pequeÃ±as no aprovechan el paralelismo (overhead > ganancia).
* **Pthreads** requiere mÃ¡s cÃ³digo y manejo manual.

---

## ğŸ¯ Conclusiones

La programaciÃ³n paralela tiene un **impacto positivo en el filtrado de imÃ¡genes**:

* **OpenMP** se consolida como la mejor opciÃ³n por su sencillez y buen speedup.
* **Pthreads** ofrece el mÃ¡ximo control y rendimiento, pero es mÃ¡s complejo.
* **MPI** es Ãºtil solo en sistemas realmente distribuidos, no en entornos simulados.

---

## ğŸ§ª Reproducir Experimentos

```bash
# Benchmark automÃ¡tico
./benchmark_all_versions.sh

# Resultados
cat benchmark_results/performance_comparison.csv
```

---

## ğŸ« Contexto AcadÃ©mico

* Curso: **ProgramaciÃ³n Paralela**
* InstituciÃ³n: \[Tu Universidad]
* Profesor: \[Nombre del Profesor]
* Semestre: \[2025-1]

---

## ğŸ“š Referencias

* Pacheco, P. S. (2011). *An Introduction to Parallel Programming*.
* OpenMP ARB (2018). *OpenMP Application Programming Interface*.
* MPI Forum (2015). *Message Passing Interface Standard*.
* Netpbm Utilities Documentation.

---

## ğŸ“„ Licencia

Licencia acadÃ©mica, uso exclusivo educativo.

---

## ğŸ“ Contacto

Autor: \[Tu Nombre]
Email: \[[tu.email@universidad.edu](mailto:tu.email@universidad.edu)]
GitHub: \[tu-usuario]

```

---

Â¿Quieres que tambiÃ©n te lo convierta directamente en **PDF estilo informe acadÃ©mico** con portada, o prefieres dejar este **README en formato Markdown** para tu repositorio de GitHub?
```
