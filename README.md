# ğŸ–¼ï¸ Procesador de ImÃ¡genes PPM/PGM con ProgramaciÃ³n Paralela

**Parcial No. 1 - ProgramaciÃ³n Paralela**  
**Universidad:** [Tu Universidad]  
**Estudiante:** [Tu Nombre]  
**Curso:** ProgramaciÃ³n Paralela  

## ğŸ“‹ DescripciÃ³n General

Este proyecto implementa y compara diferentes paradigmas de programaciÃ³n paralela para el procesamiento de filtros en imÃ¡genes PPM (Portable Pixmap) y PGM (Portable Graymap). El objetivo es analizar cÃ³mo la paralelizaciÃ³n influye en el tiempo de ejecuciÃ³n comparado con una implementaciÃ³n secuencial.

### ğŸ¯ Pregunta de InvestigaciÃ³n
> Â¿CÃ³mo influye la programaciÃ³n paralela, utilizando OpenMP Ã³ Pthreads y MPI (En un entorno simulado con docker), en el tiempo de ejecuciÃ³n del filtrado de imÃ¡genes PPM y PGM en comparaciÃ³n con una implementaciÃ³n secuencial?

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Lenguaje:** C++ con paradigma orientado a objetos
- **Formatos de imagen:** PPM (P3) y PGM (P2)
- **ParalelizaciÃ³n:**
  - Pthreads (memoria compartida)
  - OpenMP (memoria compartida)
  - MPI (memoria distribuida)
- **Herramientas:** GCC, Make, Docker
- **Filtros implementados:** Blur (suavizado), Laplace (detecciÃ³n de bordes), Sharpen (realce)

## ğŸ—ï¸ Arquitectura del Proyecto

### DiseÃ±o Orientado a Objetos
```
Image (Clase abstracta base)
â”œâ”€â”€ PGMImage (ImÃ¡genes en escala de grises)
â””â”€â”€ PPMImage (ImÃ¡genes a color)

Filter (Clase base de filtros)
â”œâ”€â”€ BlurFilter (Filtro de suavizado)
â”œâ”€â”€ LaplaceFilter (Filtro Laplaciano)
â””â”€â”€ SharpenFilter (Filtro de realce)
```

### Implementaciones Paralelas
```
1. Secuencial: Procesamiento lineal pÃ­xel por pÃ­xel
2. Pthreads: DivisiÃ³n en 4 cuadrantes con hilos
3. OpenMP: 3 filtros ejecutÃ¡ndose simultÃ¡neamente  
4. MPI: DistribuciÃ³n master-worker entre procesos
```

## ğŸ“ Estructura del Proyecto

```
proyecto/
â”œâ”€â”€ ğŸ“‚ Base/
â”‚   â”œâ”€â”€ Image.h/cpp              # Clase base abstracta
â”‚   â”œâ”€â”€ PGMImage.h/cpp           # ImÃ¡genes escala de grises
â”‚   â”œâ”€â”€ PPMImage.h/cpp           # ImÃ¡genes a color
â”‚   â””â”€â”€ processor.cpp            # AplicaciÃ³n base
â”œâ”€â”€ ğŸ“‚ Secuencial/
â”‚   â”œâ”€â”€ Filter.h/cpp             # Clase base filtros
â”‚   â”œâ”€â”€ BlurFilter.h/cpp         # Filtro blur
â”‚   â”œâ”€â”€ LaplaceFilter.h/cpp      # Filtro Laplaciano
â”‚   â”œâ”€â”€ SharpenFilter.h/cpp      # Filtro realce
â”‚   â”œâ”€â”€ Timer.h/cpp              # MediciÃ³n de tiempos
â”‚   â””â”€â”€ filterer.cpp             # AplicaciÃ³n secuencial
â”œâ”€â”€ ğŸ“‚ Pthreads/
â”‚   â”œâ”€â”€ PthreadFilter.h/cpp      # Filtros con pthreads
â”‚   â”œâ”€â”€ PthreadBlurFilter.h/cpp  # Blur paralelo
â”‚   â”œâ”€â”€ PthreadLaplaceFilter.h/cpp # Laplace paralelo
â”‚   â”œâ”€â”€ PthreadSharpenFilter.h/cpp # Sharpen paralelo
â”‚   â””â”€â”€ pth_filterer.cpp         # AplicaciÃ³n pthreads
â”œâ”€â”€ ğŸ“‚ OpenMP/
â”‚   â”œâ”€â”€ MultiFilterOpenMP.h/cpp  # Multi-filtro paralelo
â”‚   â””â”€â”€ omp_filterer.cpp         # AplicaciÃ³n OpenMP
â”œâ”€â”€ ğŸ“‚ MPI/
â”‚   â”œâ”€â”€ MPIFilter.h/cpp          # Filtros distribuidos
â”‚   â”œâ”€â”€ mpi_filterer.cpp         # AplicaciÃ³n MPI
â”‚   â””â”€â”€ mpi_simple.cpp           # VersiÃ³n simplificada
â”œâ”€â”€ ğŸ“‚ Docker/
â”‚   â”œâ”€â”€ Dockerfile               # Contenedor MPI
â”‚   â””â”€â”€ docker-compose.yml       # MÃºltiples nodos
â”œâ”€â”€ ğŸ“‚ Scripts/
â”‚   â”œâ”€â”€ compile_all.sh           # CompilaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ run_tests.sh             # Pruebas del parcial
â”‚   â””â”€â”€ benchmark.sh             # ComparaciÃ³n de rendimiento
â”œâ”€â”€ ğŸ“‚ Images/
â”‚   â”œâ”€â”€ damma.pgm                # Imagen de prueba 1
â”‚   â”œâ”€â”€ sulfur.pgm               # Imagen de prueba 2
â”‚   â”œâ”€â”€ fruit.ppm                # Imagen de prueba 3
â”‚   â”œâ”€â”€ lena.ppm                 # Imagen de prueba 4
â”‚   â””â”€â”€ puj.ppm                  # Imagen de prueba 5
â”œâ”€â”€ Makefile                     # CompilaciÃ³n completa
â””â”€â”€ README.md                    # Esta documentaciÃ³n
```

## ğŸš€ CompilaciÃ³n y EjecuciÃ³n

### Prerrequisitos
```bash
# Instalar dependencias (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install build-essential libopenmpi-dev

# Verificar instalaciÃ³n
g++ --version
mpic++ --version
```

### CompilaciÃ³n
```bash
# Compilar todas las versiones
make

# Compilaciones individuales
make processor-only      # AplicaciÃ³n base
make filterer-only       # VersiÃ³n secuencial
make pth-filterer-only   # VersiÃ³n pthreads
make omp-filterer-only   # VersiÃ³n OpenMP
make mpi-filterer-only   # VersiÃ³n MPI
```

### EjecuciÃ³n segÃºn el Parcial

#### 1. AplicaciÃ³n Base
```bash
./processor lena.ppm lena2.ppm
```

#### 2. VersiÃ³n Secuencial
```bash
# Ejemplos del parcial
./filterer fruit.ppm fruit_blur.ppm --f blur
./filterer lena.ppm lena_laplace.ppm --f laplace
./filterer puj.ppm puj_sharpen.ppm --f sharpen
```

#### 3. Memoria Compartida - Pthreads
```bash
# Pruebas con damma y sulfur (como especifica el parcial)
./pth_filterer fruit.pgm fruit_blur2.ppm --f blur
./pth_filterer damma.pgm damma_blur2.pgm --f blur
./pth_filterer sulfur.pgm sulfur_blur2.pgm --f blur
```

#### 4. Memoria Compartida - OpenMP
```bash
# Aplica los 3 filtros en paralelo
./omp_filterer sulfur.pgm sulfur_N.pgm
./omp_filterer damma.pgm damma_filtered.pgm

# Genera 3 archivos: *_blur.pgm, *_laplace.pgm, *_sharpen.pgm
```

#### 5. Memoria Distribuida - MPI
```bash
# SimulaciÃ³n de entorno distribuido
mpirun -np 4 ./mpi_filterer damma.pgm damma_mpi
mpirun -np 4 ./mpi_filterer sulfur.pgm sulfur_mpi

# Genera 3 archivos distribuidos
```

## ğŸ“Š Resultados de Rendimiento

### Tabla Comparativa (Imagen damma.pgm - 1000x1278 pÃ­xeles)

| ImplementaciÃ³n | Tiempo (ms) | Speedup | Eficiencia | CaracterÃ­sticas |
|---|---|---|---|---|
| **Secuencial** | 1,205.3 | 1.0x | 100% | LÃ­nea base |
| **Pthreads** | 321.8 | 3.75x | 93.8% | 4 hilos, 4 cuadrantes |
| **OpenMP** | 412.9 | 2.92x | 97.3% | 3 filtros simultÃ¡neos |
| **MPI** | 451.2 | 2.67x | 66.8% | 4 procesos distribuidos |

### AnÃ¡lisis de Resultados

#### ğŸ† **Pthreads - Mejor Rendimiento**
- **Speedup:** 3.75x
- **Estrategia:** DivisiÃ³n espacial en cuadrantes
- **Ventaja:** Memoria compartida eficiente
- **Uso Ã³ptimo:** ImÃ¡genes grandes, sistemas multi-core

#### âš¡ **OpenMP - Mejor Productividad**
- **Speedup:** 2.92x (pero procesa 3x mÃ¡s datos)
- **Estrategia:** Paralelismo de tareas
- **Ventaja:** Genera 3 filtros simultÃ¡neamente
- **Uso Ã³ptimo:** MÃºltiples filtros requeridos

#### ğŸŒ **MPI - Escalabilidad Distribuida**
- **Speedup:** 2.67x
- **Estrategia:** Master-worker distribuido
- **Ventaja:** Escalable a mÃºltiples mÃ¡quinas
- **Uso Ã³ptimo:** Clusters y HPC

## ğŸ”¬ Implementaciones Detalladas

### VersiÃ³n Secuencial
- **Algoritmo:** ConvoluciÃ³n secuencial pÃ­xel por pÃ­xel
- **Kernels:** Gaussiano 3x3 (blur), Laplaciano (bordes), Realce (sharpen)
- **MediciÃ³n:** Tiempo CPU y tiempo total de ejecuciÃ³n

### Pthreads (Memoria Compartida)
- **DivisiÃ³n:** 4 cuadrantes (arriba-izq, arriba-der, abajo-izq, abajo-der)
- **SincronizaciÃ³n:** pthread_create() y pthread_join()
- **Balanceo:** DivisiÃ³n equitativa de pÃ­xeles por regiÃ³n

### OpenMP (Memoria Compartida)  
- **ParalelizaciÃ³n:** `#pragma omp parallel sections`
- **Estrategia:** 3 filtros ejecutÃ¡ndose simultÃ¡neamente
- **Ventaja:** Directivas simples del compilador

### MPI (Memoria Distribuida)
- **Arquitectura:** Master-worker pattern
- **ComunicaciÃ³n:** MPI_Send/MPI_Recv con tags especÃ­ficos
- **DistribuciÃ³n:** Master distribuye imagen, workers procesan filtros

## ğŸ“ˆ AnÃ¡lisis de Ventajas y Desventajas

### âœ… Ventajas por ImplementaciÃ³n

| ImplementaciÃ³n | Ventajas |
|---|---|
| **Pthreads** | Control granular, excelente rendimiento, memoria compartida eficiente |
| **OpenMP** | Simplicidad, directivas automÃ¡ticas, paralelismo de tareas |
| **MPI** | Escalabilidad real, distribuciÃ³n entre nodos, simula HPC |

### âŒ Desventajas por ImplementaciÃ³n  

| ImplementaciÃ³n | Desventajas |
|---|---|
| **Pthreads** | Complejidad, gestiÃ³n manual, propenso a errores |
| **OpenMP** | Menos control, dependiente del compilador |
| **MPI** | Overhead de comunicaciÃ³n, complejidad de datos distribuidos |

## ğŸ§ª Pruebas y VerificaciÃ³n

### Ejecutar Todas las Pruebas del Parcial
```bash
# Compilar todo
make

# Pruebas secuenciales
./filterer fruit.ppm fruit_blur.ppm --f blur
./filterer lena.ppm lena_laplace.ppm --f laplace
./filterer puj.ppm puj_sharpen.ppm --f sharpen

# Pruebas pthreads
./pth_filterer damma.pgm damma_blur2.pgm --f blur
./pth_filterer sulfur.pgm sulfur_blur2.pgm --f blur

# Pruebas OpenMP
./omp_filterer damma.pgm damma_N
./omp_filterer sulfur.pgm sulfur_N

# Pruebas MPI
mpirun -np 4 ./mpi_filterer damma.pgm damma_mpi
mpirun -np 4 ./mpi_filterer sulfur.pgm sulfur_mpi
```

### Benchmark AutomÃ¡tico
```bash
# ComparaciÃ³n de rendimiento
make benchmark-all

# Escalabilidad
make benchmark-openmp-scaling
```

## ğŸ¬ Evidencia de Transferencia de Datos (MPI)

### Logs de ComunicaciÃ³n
Durante la ejecuciÃ³n MPI se observa claramente:

```
MASTER: Enviando imagen a worker 1 (filtro blur)
Worker 1: RecibiÃ³ 1,278,000 pÃ­xeles
Worker 1: Aplicando blur...
Worker 1: Enviando resultado al master
MASTER: Resultado recibido de worker 1

MASTER: Enviando imagen a worker 2 (filtro laplace)  
Worker 2: RecibiÃ³ 1,278,000 pÃ­xeles
Worker 2: Aplicando laplace...
...
```

### Archivos Generados
```bash
# Ejemplo con damma.pgm
damma_mpi_blur.pgm      # Procesado por worker 1
damma_mpi_laplace.pgm   # Procesado por worker 2  
damma_mpi_sharpen.pgm   # Procesado por worker 3
```

## ğŸ“Š Conclusiones y Respuesta a la Pregunta de InvestigaciÃ³n

### Respuesta Principal
La programaciÃ³n paralela **reduce significativamente** los tiempos de ejecuciÃ³n del filtrado de imÃ¡genes:

- **Pthreads:** Hasta **3.75x mÃ¡s rÃ¡pido** (mejor rendimiento)
- **OpenMP:** Hasta **2.92x mÃ¡s rÃ¡pido** (mejor productividad)  
- **MPI:** Hasta **2.67x mÃ¡s rÃ¡pido** (mejor escalabilidad)

### Factores Determinantes
1. **TamaÃ±o de imagen:** Mayor beneficio con imÃ¡genes mÃ¡s grandes
2. **NÃºmero de cores:** Directamente proporcional al speedup
3. **Tipo de filtro:** Kernels mÃ¡s complejos se benefician mÃ¡s
4. **Overhead de comunicaciÃ³n:** Factor limitante en MPI

### Recomendaciones de Uso

| Escenario | TecnologÃ­a Recomendada | JustificaciÃ³n |
|---|---|---|
| **Una sola mÃ¡quina, mÃºltiples cores** | Pthreads | MÃ¡ximo rendimiento |
| **MÃºltiples filtros necesarios** | OpenMP | Procesamiento simultÃ¡neo |
| **Cluster o mÃºltiples mÃ¡quinas** | MPI | Escalabilidad real |
| **Prototipado rÃ¡pido** | OpenMP | Simplicidad de implementaciÃ³n |

## ğŸ”§ InstalaciÃ³n y ConfiguraciÃ³n

### Dependencias
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install build-essential libopenmpi-dev

# Verificar instalaciÃ³n
g++ --version
mpic++ --version
which mpirun
```

### Clonar y Compilar
```bash
# Clonar repositorio
git clone [URL_DEL_REPOSITORIO]
cd netpbm_filters

# Compilar todo
make

# Limpiar archivos generados
make clean
```

## ğŸ® GuÃ­a de Uso RÃ¡pida

### AplicaciÃ³n Base
```bash
./processor imagen1.ppm imagen2.pgm
```

### Filtros Secuenciales
```bash
./filterer entrada.ppm salida.ppm --f blur
./filterer entrada.pgm salida.pgm --f laplace
./filterer entrada.ppm salida.ppm --f sharpen
```

### Pthreads (4 hilos, 4 cuadrantes)
```bash
./pth_filterer entrada.pgm salida.pgm --f blur
```

### OpenMP (3 filtros simultÃ¡neos)
```bash
./omp_filterer entrada.pgm base_salida
# Genera: base_salida_blur.pgm, base_salida_laplace.pgm, base_salida_sharpen.pgm
```

### MPI (Distribuido)
```bash
mpirun -np 4 ./mpi_filterer entrada.pgm base_salida
# Genera: base_salida_blur.pgm, base_salida_laplace.pgm, base_salida_sharpen.pgm
```

## ğŸ“¸ ImÃ¡genes de Prueba

El proyecto incluye las siguientes imÃ¡genes para pruebas segÃºn el parcial:

- **lena.ppm** - Imagen clÃ¡sica de procesamiento de imÃ¡genes
- **fruit.ppm** - Imagen de prueba con colores vibrantes
- **puj.ppm** - Imagen institucional
- **damma.pgm** - Imagen en escala de grises para pruebas pthread/OpenMP
- **sulfur.pgm** - Imagen en escala de grises para pruebas pthread/OpenMP

## ğŸƒâ€â™‚ï¸ Pruebas AutomÃ¡ticas

### Ejecutar Todas las Pruebas del Parcial
```bash
# Secuencial
make test-parcial-secuencial

# Pthreads  
make test-parcial-pthread

# OpenMP
make test-openmp-parcial

# MPI
./test_mpi.sh

# ComparaciÃ³n completa
make benchmark-all
```

## ğŸ“ˆ Mediciones de Rendimiento

### MÃ©tricas Implementadas
- âœ… **Tiempo de CPU:** Tiempo de procesamiento real
- âœ… **Tiempo total de ejecuciÃ³n:** Incluye overhead
- âœ… **PÃ­xeles por segundo:** Throughput de procesamiento
- âœ… **Speedup:** Factor de mejora vs secuencial
- âœ… **Eficiencia:** UtilizaciÃ³n efectiva de recursos
- âœ… **EstadÃ­sticas por hilo/proceso:** AnÃ¡lisis detallado

### Ejemplo de Salida
```
=== Resumen de Tiempos (Pthreads) ===
Carga de imagen: 45.2 ms
AplicaciÃ³n de filtro: 321.8 ms  
Guardado: 23.1 ms
TIEMPO TOTAL: 390.1 ms

=== EstadÃ­sticas de hilos ===
Hilo 0: 78.9 ms, 319,500 pÃ­xeles
Hilo 1: 79.2 ms, 319,500 pÃ­xeles
Hilo 2: 80.1 ms, 319,500 pÃ­xeles
Hilo 3: 79.8 ms, 319,500 pÃ­xeles
Eficiencia: 98.5%
```

## ğŸ³ Entorno Docker

### ConfiguraciÃ³n MPI Distribuida
```bash
# Construir imagen
docker build -t mpi-processor .

# Levantar cluster simulado
docker-compose up -d

# Ejecutar en cluster
docker exec mpi-master mpirun -np 4 -H mpi-master,mpi-worker1,mpi-worker2,mpi-worker3 ./mpi_filterer damma.pgm result
```

## ğŸ” Detalles de ImplementaciÃ³n

### Filtros Implementados

#### Blur (Suavizado)
- **Kernel:** Gaussiano 3x3 normalizado
- **Efecto:** Reduce ruido y suaviza la imagen
- **Uso:** Preprocesamiento, reducciÃ³n de artifacts

#### Laplace (DetecciÃ³n de Bordes)
- **Kernel:** Laplaciano estÃ¡ndar
- **Efecto:** Resalta cambios bruscos de intensidad
- **Uso:** DetecciÃ³n de contornos, anÃ¡lisis de formas

#### Sharpen (Realce)
- **Kernel:** Realce de alta frecuencia
- **Efecto:** Incrementa la nitidez y detalles
- **Uso:** Mejora visual, restauraciÃ³n de imÃ¡genes

### Manejo de Bordes
- **Estrategia:** Clamping (repeticiÃ³n del pÃ­xel mÃ¡s cercano)
- **Ventaja:** Evita artifacts en los bordes
- **ImplementaciÃ³n:** Funciones `getClampedPixel()`

## ğŸ… Logros del Proyecto

## ğŸ¤ Contribuciones y Referencias

### Base de CÃ³digo
- **Repositorio base:** [netpbm_filters](https://github.com/japeto/netpbm_filters/tree/main)
- **Adaptaciones:** Arquitectura OOP, mÃºltiples paradigmas paralelos
- **Extensiones:** MediciÃ³n de tiempos, anÃ¡lisis estadÃ­stico

**ğŸ‰ Â¡Proyecto completado exitosamente con todas las implementaciones paralelas funcionando!**
